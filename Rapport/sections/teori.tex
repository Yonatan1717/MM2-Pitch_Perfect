\section{Teori}
Målet i dette prosjektet er å utvikle et system som analyserer lydsignaler i sanntid ved hjelp av Fourier-metoder. For å gjøre dette trenger vi en grunnleggende forståelse av Fourier-transformasjonen og hvordan diskrete varianter (DFT/FFT) anvendes på digitale lydsignaler. Videre beskriver vi Short-Time Fourier Transform (STFT) for tids-frekvensanalyse, og diskuterer praktiske hensyn som vinduvalg, frekvensoppløsning, lekkasje og latens.\\[1em]
Kapittelet gir først en kort teori for FT/DFT/FFT og deres sammenhenger. Deretter presenterer vi STFT-formuleringen og sentrale designvalg (vindustype, rammelengde $N$, hop-size $H$). Til slutt omtaler vi metoder for støyhåndtering og presisjonsheving (RMS-gating, og parabolsk toppinterpolasjon) som danner grunnlaget for analysen brukt i prosjektet.
\subsection{Fourier-transformasjonen (FT)}
Fourier-transformasjonen er et matematisk verktøy som brukes til å analysere signaler i frekvensdomenet. Den gir en måte å representere et tidsavhengig signal som en sum av sinusformede bølger med forskjellige frekvenser og amplituder. Den kontinuerlige Fourier-transformasjonen (FT) er definert som \cite{FT}:
\begin{equation}
X(f) = \int_{-\infty}^{\infty} x(t) e^{-\jj 2 \pi f t} dt = \mathcal{F}\{x(t)\}
\end{equation}
Hvor hvor $X(f)$ er frekvensspekteret til tidsdomenet signalet $x(t)$, og $f$ er frekvensen. Dette gir sammenhengen:
\[
    X(f) = \mathcal{F}\{x(t)\} \Leftarrow \Rightarrow x(t) = \mathcal{F}^{-1}\{X(f)\}
\]
Noen nyttige egenskaper ved Fourier-transformasjonen inkluderer linearitet, tids- og frekvensforskyvning, samt konvolusjonsteoremet. Disse egenskapene gjør det lettere å analysere og manipulere signaler i frekvensdomenet:\\

\textbf{Linearitet:} 
\[
    \qquad  \qquad \ \ \mathcal{F}\{a x_1(t) + b x_2(t)\} = a X_1(f) + b X_2(f)
\]
Linearitetsprinsippet sier at Fourier-transformasjonen av en kombinasjon av signaler er lik den samme kombinasjonen av Fourier-transformasjonene til hvert signal. Dette gjør det lettere å analysere komplekse signaler ved å dele dem opp i enklere deler.\\

\textbf{Tidsforskyvning:}
\[
    \ \ \mathcal{F}\{x(t - t_0)\} = X(f) e^{-\jj 2 \pi f t_0}
\]
Tidsforskyvning viser hvordan et signal som flyttes i tid gir en faseendring i frekvensdomenet.\\
\clearpage
\noindent
\textbf{Frekvensforskyvning:} 
\[
    \mathcal{F}\{x(t) e^{\jj 2 \pi f_0 t}\} = X(f - f_0)
\]
Frekvensforskyvning beskriver hvordan modulasjon i tidsdomenet flytter signalets spektrum i frekvensdomenet.\\
 
\textbf{Konvolusjonsteoremet:} 
\[
    \qquad \ \mathcal{F}\{x_1(t) * x_2(t)\} = X_1(f) \cdot X_2(f)
\]
Konvolusjonsteoremet brukes til å analysere hvordan systemer filtrerer signaler, fordi konvolusjon i tid tilsvarer multiplikasjon i frekvens. \\

\textbf{Parsevals teorem:} 
\[
    \ \quad \int_{-\infty}^{\infty} |x(t)|^2 dt = \int_{-\infty}^{\infty} |X(f)|^2 df
\]
Til slutt beskriver Parsevals teorem sammenhengen mellom energi i tids- og frekvensdomenet, og brukes til å beregne eller sammenligne signalenergi på tvers av domener.

\subsection{Diskret Fourier Transform (DFT)}
Den diskrete Fourier-transformasjonen (DFT) er en numerisk metode for å beregne Fourier-transformasjonen av et diskret signal. Det at et signal er diskret betyr at det er representert som en sekvens av prøver tatt på bestemte tidspunkter. DFT brukes ofte i digital signalbehandling for å analysere frekvensinnholdet i digitale signaler. DFT er definert som \cite{DFT}:
\begin{equation}
    X[k] = \sum_{n=0}^{N-1} x[n] W^{kn}, \quad k = 0, 1, \ldots, N-1, \quad W = e^{-\jj 2 \pi / N}
    \label{DFT}
\end{equation}
Hvor $N$ er antall prøver i signalet $x[n]$. DFT gir et diskret frekvensspekter $X[k]$ som representerer signalets frekvensinnhold. DFT kan også uttrykkes i matriseform som:
\begin{equation}
    \mathbf{X} = \mathbf{A} \cdot \mathbf{x[n]}
\end{equation}
Hvor $\mathbf{x}$ er en vektor av tidsdomenet signalprøver, $\mathbf{X}$ er frekvensdomenet representasjonen, og $\mathbf{A}$ er en $N \times N$ matrise med elementene:
\[
    W_{kn} = W^{kn} = e^{-\jj 2 \pi kn / N}
\]
For å vurdere hvor effektiv en DFT-beregning er, ser vi på tidskompleksiteten, altså antall (elementære) operasjoner som funksjon av antall prøver $N$. DFT har en tidskompleksitet på $O(N^2)$, noe som kan være ineffektivt for lange signaler. Dette kan vi se ved at for hver av de $N$ frekvenskomponentene må vi utføre en sum over $N$ tidsprøver, noe som gir totalt $N \times N = N^2$ operasjoner. Altså:
\[
    B(N) = (k_{max} + 1) \cdot (n_{max}+1) = N \cdot N = N^2
\]
En slik kompleksiteten har den følgende øknings om vist i figuren under:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./Media/DFT_Complexity.png}
    \caption{Tidskompleksitet for DFT som funksjon av antall prøver $N$.}
    \label{fig:DFT_Complexity}
\end{figure}
\noindent
\subsection{Fast Fourier Transform (FFT)}
Som beskrevet tidligere har DFT en tidskompleksitet på $O(N^2)$, noe som kan være ineffektivt for lange signaler siden det tar lengre tid. For å løse dette problemet ble Fast Fourier Transform (FFT) utviklet. FFT er en familie av effektive algoritmer for å beregne DFT med betydelig redusert tidskompleksitet. Den reduserer tidskompleksiteten fra $O(N^2)$ til $O(N \log N)$ ved å utnytte symmetrier i DFT og dele opp beregningene i mindre deler. FFT er spesielt nyttig for lange signaler, der den betydelige reduksjonen i beregningstid kan være avgjørende for sanntidsapplikasjoner slik som lyd- og bildebehandling. Blant de mest kjente FFT-algoritmene er Cooley-Tukey-algoritmen (Radix-2, som baserer seg på Danielson-Lanczos-lemmaet), som vi skal bruke som et eksempel \cite{FFT}. Notasjonen vi bruker er formelen for DFT \eqref{DFT}:
\[
    X[k] = \sum_{n=0}^{N-1} x[n] W_{N}^{kn}, \quad W_{N} = e^{-\jj 2 \pi / N}
\]
Her bruker vi $W_{N}$ for å indikere $N$-punkt DFT. Denne kan deles opp i to summer, en for partallsindekserte elementer og en for oddetallsindekserte elementer:
\[
    X[k] = \sum_{m=0}^{N/2-1} x[2m] W_{N}^{k(2m)} + \sum_{m=0}^{N/2-1} x[2m+1] W_{N}^{k(2m+1)}
\]
Ved å faktorisere ut $W_{N}^{k}$ fra den andre summen får vi: 
\[
    X[k] = \sum_{m=0}^{N/2-1} x[2m] W_{N}^{2km} + W_{N}^{k} \sum_{m=0}^{N/2-1} x[2m+1] W_{N}^{2km}
\]
Legg merke til at:
\[
    W_{N}^{2km} = e^{-\jj 2 \pi (2km) / N} = e^{-\jj 2 \pi km / (N/2)} = W_{N/2}^{km}
\]
Som tilsvarer DFT av lengde $N/2$. Vi kan derfor definere to nye DFT-er:
\[
    E[k] = \sum_{m=0}^{N/2-1} x[2m] W_{N/2}^{km}, \quad O[k] = \sum_{m=0}^{N/2-1} x[2m+1] W_{N/2}^{km}
\]
Dermed kan vi skrive:
\[
    X[k] = E[k] + W_{N}^{k} O[k], \quad k = 0, 1, \ldots, N/2 - 1
\]
For $k = 0 \ldots N/2 - 1$ får vi også (pga periodisiteten til DFT):
\[
    X[k + N/2] = E[k] - W_{N}^{k} O[k], \quad k = 0, 1, \ldots, N/2 - 1
\]
Grunnen til at vi får minus-tegnet i den andre ligningen er at:
\[
    W_N^{k + N/2} = W_N^k \cdot W_N^{N/2} = W_N^k \cdot e^{-\jj \pi} = -W_N^k
\]
Vi har da butterfly-operasjonen:
\[
    X[k] = E[k] + W_{N}^{k} O[k], \qquad X[k + N/2] = E[k] - W_{N}^{k} O[k], \quad k = 0, 1, \ldots, N/2 - 1
\]

\vspace{2em}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./Media/butterfly.png}
    \caption{Butterfly-operasjonen i FFT-algoritmen $G(k) = E(k)$ og $H(k)=O(k)$.}
    \label{fig:butterfly}
\end{figure}

\vspace{2em}

\noindent
Dette betyr at vi kan beregne en $N$-punkt DFT ved å beregne to $N/2$-punkt DFT-er, noe som reduserer antall nødvendige beregninger betydelig. Ved å gjenta denne prosessen rekursivt til vi når DFT-er av lengde 2, oppnår vi den effektive FFT-algoritmen med tidskompleksitet $O(N \log N)$. Dette kan vi se ved å la $B(N)$ være antall grunnoperasjoner for en $N$-punkts FFT. Da har vi:
\[
    B(N) = aB(N/b) + cN, \quad B(1) = c_0
\]
hvor $a$ er antall delproblemer (her $2$), $b$ er faktoren vi deler problemet med (her $2$), og $cN$ representerer den lineære tiden som kreves for å kombinere resultatene, $c$ er altså en konstant. Dette gir:
\[
    B(N) = 2B(N/2) + cN
\]
Siden $N=2^m$ har rekursjonstreet $m=\log_2 N$ nivåer. Hvert nivå koster lineær tid i $N$ (ca. $N/2$ \emph{butterflies}), vi får derfor totalt:
\[
    B(N)=cN\log_2 N=O(N\log_2 N).
\]
Dette gjør FFT til en svært effektiv metode for å beregne DFT, spesielt for store datasett. Dette illustreres i figur \ref{fig:DFT_Complexity}.

\vspace{2em}

\begin{figure}[H]
    \centering
    \includegraphics[width=.9\textwidth]{./Media/DFT_Complexity_VS_FFT.png}
    \caption{Operasjoner for DFT vs FFT som funksjon av antall prøver $N$.}
    \label{fig:FFT_Complexity}
\end{figure}
\noindent
\subsection{Short-Time Fourier Transform (STFT)}
STFT er en utvidelse av Fourier-transformasjonen som gjør det mulig å analysere tidsvarierende signaler. STFT deler signalet i overlappende rammer og beregner en DFT for hver ramme. Slik får vi frekvensinnholdet som funksjon av tid. Matematisk defineres STFT som vist under \cite{STFT}:
\[
    \text{STFT}\{x[n]\}(m, k) = \sum_{n=-\infty}^{\infty} x[n+mH] w[n] e^{-\jj 2 \pi k n / N}
\]
hvor \(w[n]\) er vindusfunksjonen, \(m\) er tidsindeksen for rammen, \(H\) er hop-størrelsen, og \(k\) er frekvensindeksen. Ved å bruke STFT kan vi visualisere tids-frekvensrepresentasjonen av signalet, noe som er nyttig i mange applikasjoner som talegjenkjenning, musikkprosessering og mer.




\subsubsection{Rammelengde}
Rammelengden ($N$) refererer til antall prøver i hver ramme som analyseres av STFT. Valget av rammelengde påvirker både tids- og frekvensoppløsningen. Generelt gjelder følgende forhold:

\begin{enumerate}[label=\null]
    \item \textbf{Tidsoppløsning:} Jo kortere rammelengde, desto bedre tidsoppløsning. Noe som gjør at applikasjonen kan fange raske endringer i signalet og oppleves mer responsivt, men dette kan føre til redusert frekvensoppløsning.
    
    \vspace{1em}

    \item \textbf{Frekvensoppløsning:} Jo lengre rammelengde, desto bedre frekvensoppløsning. Noe som gir mer presis frekvensanalyse, men dette kan føre til redusert tidsoppløsning.
    \[
        \Delta f = \frac{f_s}{N} \quad \text{[Hz]}
    \]
    hvor \(f_s\) er samplingsfrekvensen.
\end{enumerate}
\subsubsection{Hop-størrelse}
I STFT er hop-størrelse ($H$) avstanden mellom påfølgende rammer i tid. Det bestemmer hvor mye hver ramme overlapper med den forrige. En mindre hop-størrelse gir mer overlapping, noe som kan forbedre tidsoppløsningen, men øker beregningskostnaden. En større hop-størrelse reduserer overlappingen, noe som kan føre til tap av informasjon om raske endringer i signalet. Valget av hop-størrelse avhenger av applikasjonen og ønsket balanse mellom tids- og frekvensoppløsning. En vanlig praksis er å bruke en hop-størrelse som er en brøkdel av rammelengden, for eksempel:
\[
    H = \frac{N}{4} \quad \text{eller} \quad H = \frac{N}{2}
\]
Noe som gir 75\% eller 50\% overlapping mellom rammer. Dette gir en god balanse mellom tids- og frekvensoppløsning for mange applikasjoner. Ved disse valgene vil tidsoppløsningen være:
\[
    \Delta t = \frac{H}{f_s} \quad \text{[s]}
\]
\noindent
\subsection{Vindusfunksjoner}
Fourier-transform analyserer et tidskontinuerlig signal på formen:
\[
x(t): \mathbb{R} \rightarrow \mathbb{R}
\]

\noindent
I praksis er lydsignaler tidsvarierende og ikke uendelig lange, og derfor deles de inn i kortere tidssegmenter (rammer). Hver ramme behandles som om den gjentas periodisk, slik at vi kan analysere hvilke frekvenser som dominerer innenfor et gitt tidsrom. Slik kan man finne hvilke frekvenser som er til stede i et gitt tidsintervall $\Delta t$ [s].
Når signalet deles i rammer oppstår skarpe overganger ved kantene. Disse diskontinuitetene fører til at energien i spekteret “lekker” utover flere frekvenser, noe som kalles \textit{spectral leakage}. Dette skjer fordi brå avslutning i tidsdomenet (diskontinuiteter) innebærer et bredt frekvensinnhold, slik at energien i spekteret lekker utover flere frekvenskomponenter \cite{Windowing}. Et eksempel på dette vises i figuren under, der et sinusoidalt signal blir avkuttet brått ved rammens start og slutt:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{./Media/signal_uten_vindu_diskontoniuitete.png}
    \caption{Eksempel på spectral leakage i et sinusoidalt signal ved bruk av rektangulært vindu}
    \label{fig:spectral_leakage}
\end{figure}
For å redusere lekkasje benytter man vindusfunksjoner, som ganges med signalet og gradvis demper det mot null ved rammekantene:
\[
    x_w(t) = x(t) \cdot w(t)
\]
Dette gjør overgangen mykere og reduserer diskontinuiteter i tidsdomenet (Se figur \ref{fig:window_func}). Siden dette er multiplikasjon i tid gir det konvolusjon i frekvensdomenet:
\[
    X_w(f) = X(f) * W(f)
\]
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{./Media/signal_med_vindu_ikke_diskontoniuitete.png}
    \caption{Eksempel på Hanning-vindu som jevner ut rammekantene sammenlignet med rektangulært vindu}
    \label{fig:window_func}
\end{figure}
En viktig ting å merke seg er at noen vindusfunksjoner (de fleste) introduserer en faktore som kalles \textit{coherent gain}. Dette refererer til den endringen i amplituden av signalet som skjer når det blir vinduet. For eksempel, Hanning-vinduet reduserer toppamplituden til omtrent 50\% av den opprinnelige verdien, noe som må tas i betraktning ved analyse av signalet. Tabell \ref{tab:window_functions} oppsummerer noen vanlige vindusfunksjoner og deres egenskaper.
\begin{table}[H]
    \centering
    \caption{Sammenligning av vanlige vindusfunksjoner}
    \begin{tabular}{|l|l|l|l|}
        \hline
        \textbf{Vindu} & \textbf{Formel} & \textbf{CG} & \textbf{Hovedlobebredde} \\
        \hline
        Rektangulært & $w[n] = 1$ & 1.0 & $\approx 2 \Delta f$ \\
        Hann & $w[n] = 0.5 \left(1 - \cos\left(\frac{2\pi n}{N-1}\right)\right)$ & 0.5 & $\approx 4 \Delta f$ \\
        Hamming & $w[n] = 0.54 - 0.46 \cos\left(\frac{2\pi n}{N-1}\right)$ & 0.54 & $\approx 4 \Delta f$ \\
        Blackman & $w[n] = 0.42 - 0.5 \cos\left(\frac{2\pi n}{N-1}\right) + 0.08 \cos\left(\frac{4\pi n}{N-1}\right)$ & 0.42 & $\approx 6 \Delta f$ \\
        \hline
    \end{tabular}
    
    \label{tab:window_functions}
\end{table}


Figur \ref{fig:frekvens_spekter} viser at Hanning-vinduet gir betydelig lavere sidelober i frekvensdomenet enn det rektangulære vinduet. Dette betyr at mindre energi lekker over i nabofrekvenser, og frekvensanalysen blir mer presis.


\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{./Media/frekvens_spekter.png}
    \caption{Sammenligning av Hanning-vinduet og rektangulært vindu i frekvensdomenet}
    \label{fig:frekvens_spekter}
\end{figure}
Frekvensdomenet plottet i figur \ref{fig:frekvens_spekter} er basert på følgende tidsfunksjon:
\[
    x(t) = \sin(25t), \quad t \in [0, 5]
\]
Slik kan vi oppsummere effekten av vindusfunksjonene:
\begin{enumerate}[label=\null]
    \item \textbf{Hann-vinduet:} Reduserer lekkasje betydelig sammenlignet med det rektangulære vinduet. Dette skyldes at vinduet taperer rammen mot null ved endene, slik at diskontinuiteter i tidsdomenet blir mindre. Uten gain-korreksjon får man lavere toppamplitude (coherent gain $\approx$ 0,5), altså blir den målte toppen omtrent halvert. Hovedloben blir bredere ($\approx$ 4 $\Delta f$ mot $\approx$ 2 $\Delta f$ for rektangel), mens sidelobene er betydelig lavere, noe som reduserer lekkasje til andre frekvenser.
    \vspace{1em}
    \item \textbf{Rektangulært vindu:} Har skarpe overganger ved rammeendene, som gir sterke diskontinuiteter i tidsdomenet og dermed betydelig spectral leakage i frekvensdomenet. Energien fra en tone fordeles over flere frekvensbinner, noe som kan maskere svake nabotoner og forvrenge tolkningen. Hovedloben er smalere enn for Hann-vinduet ($\approx$ 2 $\Delta f$ mot $\approx$ 4 $\Delta f$), men sidelobene er høyere og avtar saktere, noe som øker lekkasjen.
\end{enumerate}
\subsubsection{Hann-vinduet}
Blant vindusfunksjoner er Hann-vinduet en av de mest brukte. Den er definert som:
\begin{equation}
    w[n]=\frac{1}{2} \left( 1 - \cos{\left( \frac{2 \pi n}{N - 1}\right)} \right)
    \label{eq:HanningVinduFunksjon}
\end{equation}
Der $N$ er antall prøver i vinduet. 
Hann-vinduet gir en god balanse mellom frekvensoppløsning og lekkasjereduksjon. 
I koden multipliseres hvert signalutdrag med et Hann-vindu før FFT-beregningen, slik at de beregnede frekvenskomponentene bedre representerer de faktiske tonene i lyden vi analyserer. Grafene under viser hvordan et signal $x(t)$ (figur \ref{fig:org_signal}) vil se ut etter et Hann-vindu transformasjonsfunksjon $w$ (figur \ref{fig:window_func2}). Resultatet er en funksjon $f \cdot w$ (figur \ref{fig:windowed_signal}):

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{./Media/org_signal.png}
    \caption{Originalt signal som funksjon $x(t)$}
    \label{fig:org_signal}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{./Media/window_func.png}
    \caption{Hann-vinduet som funksjon av $w(t)$}
    \label{fig:window_func2}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{./Media/windowd_signal.png}
    \caption{Produktet av Hann-vinduet og originalt signal}
    \label{fig:windowed_signal}
\end{figure}

\vspace{2em}

\subsubsection{Parabolsk toppinterpolasjon}
FFT gir diskrete frekvenspunkter, men den faktiske frekvensen til en tone kan og vil ofte ligge mellom to slike punkter. Parabolsk toppinterpolasjon muligjør estimasjon av toppfrekvensen mer presist ved å tilpasse en parabel rundt det største spektralpunktet og dets to naboer. Uten interpolasjon får vi en oppløsning gitt ved:
\[
    \Delta f = \frac{f_s}{N}
\]
\noindent
der $f_s$ er samplingsfrekvensen og $N$ er antall FFT-frekvenspunkter. Dette innebærer at den faktiske toppfrekvensen til et signal ligger mellom to slike FFT-frekvenspunkter. Vi benytter parabolsk toppinterpolasjon for å estimere toppfrekvensen mer presist. Prinsippet bak metoden er at toppen av FFT-magnitude-spekteret kan beskrives som en parabol rundt maksimumet. Vi kan beskrive en slik parabolsk andregradsfunksjon med tre punkter (toppunkt og to naboer) \cite{ParabolicInterpolation}. Dette uttrykkes matematisk slik under.
\[
    \delta = \frac{1}{2} \cdot \frac{M_{k-1} - M_{k + 1}}{M_{k - 1} - 2M_k + M_{k + 1}}
\]
Figur \ref{fig:parabolic_interpolation} illustrerer prinsippet bak parabolsk toppinterpolasjon, der vi bruker de tre punktene for å finne toppunktet til parabolen som best tilpasser disse punktene.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{./Media/paraoboliscInt.png}
    \caption{Illustrasjon av parabolsk toppinterpolasjon ved bruk av tre punkter nærmest toppen \cite{ParabolicInterpolation}.}
    \label{fig:parabolic_interpolation}
\end{figure}
\noindent
Der $M_k$ er amplituden ved toppunktet $(M_{k-1}, M_{k + 1})$ naboene. Den estimerte frekvensen kan da uttrykkes som:

\begin{equation}
    \hat{f}=\left( k + \delta \right) \cdot \frac{f_s}{N}
    \label{eq:estFreq}
\end{equation}

\subsection{Root Mean Square (RMS)}
RMS er nyttig når man skal måle eller sammenligne styrken til et signal over tid. Det brukes også i støyanalyse for å finne et representativt “gjennomsnittsnivå” av lydintensiteten. RMS gir et mål på den effektive verdien av amplituden til et signal, og gir dermed et bedre bilde av den opplevde energien i lyden enn et vanlig gjennomsnitt. I vårt system brukes RMS for å stabilisere frekvensanalysen ved å unngå at svak bakgrunnsstøy blir tolket som betydningsfullt signal \cite{RMS}.

Matematisk defineres RMS for et diskret signal $x[n]$ med $N$ prøver som:
\begin{equation}
    x_{RMS} = \sqrt{\frac{1}{N} \sum_{n=1}^{N} x[n]^2}
    \label{eq:RMS}
\end{equation}
\noindent
Matematisk ser vi at beregningen innebærer å kvadrere hver verdi av $x[n]$ for å gjøre alle amplituder positive, deretter tar gjennomsnittet og til slutt roten av resultatet. RMS kan brukes til å måle lydnivået over tid, sammenligne styrken til ulike signaler og redusere støy. 

